{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# sparql query a list of wikidata films and respective english wikipedia pages.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from requests_html import HTMLSession\n",
    "import datetime\n",
    "import hashlib\n",
    "import json\n",
    "import numpy\n",
    "import pandas\n",
    "import pathlib\n",
    "import pydash\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def value_extract(row, col):\n",
    "\n",
    "    # extract dictionary values. \n",
    "\n",
    "    return pydash.get(row[col], 'value')    \n",
    "    \n",
    "def sparql_query(query, service):\n",
    "\n",
    "    # send sparql request, and formulate results into a dataframe. \n",
    "\n",
    "    r = requests.get(service, params = {'format': 'json', 'query': query})\n",
    "    data = pydash.get(r.json(), 'results.bindings')\n",
    "    data = pandas.DataFrame.from_dict(data)\n",
    "    for x in data.columns:    \n",
    "        data[x] = data.apply(value_extract, col=x, axis=1)\n",
    "    return data\n",
    "\n",
    "def wikipedia_to_wikidata(row):\n",
    "\n",
    "    # retrieve wikidata id from wikipedia page title. \n",
    "\n",
    "    query = 'https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&titles='\n",
    "    query += str(row['wikipedia_actor'].replace('/wiki/', ''))\n",
    "    query += '&format=json'\n",
    "    r = requests.get(query)\n",
    "    if r.status_code == 200:\n",
    "        if str(r.text)[0] == '{':\n",
    "            r = json.loads(r.text)\n",
    "            for x in pydash.get(r, 'query.pages'):\n",
    "                return pydash.get(r, f'query.pages.{x}.pageprops.wikibase_item')\n",
    "\n",
    "def save_path(qcode):\n",
    "\n",
    "    # construct a predictable save path for csv using partial md5. \n",
    "\n",
    "    qcode_hash = hashlib.md5(qcode.encode()).hexdigest()\n",
    "    save_path = pathlib.Path.home() / 'git' / 'wikipedia-filmcredits' / 'data'\n",
    "    save_path = save_path / qcode_hash[:2] / f'{qcode}.csv'\n",
    "    return save_path\n",
    "\n",
    "def character_process(char_text):\n",
    "\n",
    "    # string processing for character names.\n",
    "\n",
    "    char = char_text.split('</a> as ')[1].replace('</li>', '') \n",
    "    for d in [',', ':', ';', ' - ', '\\[', '\\(as', '\\(credited', '\\(uncredited', '</ul>']:\n",
    "        char = char.split(d)[0].strip()\n",
    "    return BeautifulSoup(char).get_text()\n",
    "\n",
    "def character_length(row):\n",
    "\n",
    "    # filter character names over a certain length.\n",
    "\n",
    "    if len(str(row['character_name'])) > 40:\n",
    "        return 'NO CHARACTER'\n",
    "    else:\n",
    "        return row['character_name']\n",
    "\n",
    "film_list = sparql_query(\"\"\"\n",
    "    SELECT DISTINCT ?film ?link WHERE {\n",
    "        ?film wdt:P31 wd:Q11424\n",
    "        OPTIONAL {\n",
    "            ?link schema:about ?film.\n",
    "            ?link schema:inLanguage \"en\".\n",
    "            ?link schema:isPartOf <https://en.wikipedia.org/>\n",
    "        }}\"\"\", 'https://query.wikidata.org/sparql')\n",
    "\n",
    "film_list['film'] = film_list['film'].str.split('/').str[-1]\n",
    "print(len(film_list))\n",
    "film_list.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "267408\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>film</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q119889</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Aliens_in_the_Attic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q121810</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grown_Ups_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q120626</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Song_of_the_Thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q119704</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Father_(1996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q120484</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fair_Game_(1995_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      film                                               link\n",
       "0  Q119889  https://en.wikipedia.org/wiki/Aliens_in_the_Attic\n",
       "1  Q121810     https://en.wikipedia.org/wiki/Grown_Ups_(film)\n",
       "2  Q120626  https://en.wikipedia.org/wiki/Song_of_the_Thin...\n",
       "3  Q119704  https://en.wikipedia.org/wiki/The_Father_(1996...\n",
       "4  Q120484  https://en.wikipedia.org/wiki/Fair_Game_(1995_..."
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# parse cast lists, including pulling actor wikidata links where possible.\n",
    "\n",
    "film_list = film_list[:10000].to_dict('records')\n",
    "film_list = [x for x in film_list if save_path(x['film']).exists() == False]\n",
    "\n",
    "commence = datetime.datetime.now()\n",
    "for i in range(len(film_list)):\n",
    "    time.sleep(1)\n",
    "    t = (datetime.datetime.now()-commence)/(i+1)\n",
    "    time_to_finish = (((t)*(len(film_list)))+commence).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f'processed: {i+1} of {len(film_list)}; eta {time_to_finish}.')\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    cast_data = pandas.DataFrame(columns=['wikipedia_actor', 'character_name'])\n",
    "    row = film_list[i]\n",
    "    page = HTMLSession().get(str(row['link'])).text\n",
    "    page = [x for x in page.split('<h2>') if 'id=\"Cast\"' in str(x)]\n",
    "    if len(page) == 1:\n",
    "        page = page[0]\n",
    "        page = [x for x in page.split('<li>') if len(x)][1:]\n",
    "        page = [x for x in page if 'page does not exist' not in str(x)]\n",
    "        for p in page:\n",
    "            if '</a> as ' in str(p):\n",
    "                link = p.split('\" title')[0].replace('<a href=\"', '')\n",
    "                cast_data.loc[len(cast_data)] = [(link.strip()), (character_process(p))]\n",
    "    if len(cast_data):\n",
    "        cast_data['wikidata_actor'] = cast_data.apply(wikipedia_to_wikidata, axis=1)\n",
    "        cast_data['wikidata_film'] = row['film']\n",
    "        save_path(row['film']).parents[0].mkdir(parents=True, exist_ok=True)\n",
    "        cast_data.to_csv(save_path(row['film']), index=False)\n",
    "\n",
    "cast_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikipedia_actor</th>\n",
       "      <th>character_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [wikipedia_actor, character_name]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pathlib, pandas, numpy, datetime, os\n",
    "\n",
    "# group data by extraction date, plus last minute filtering and formatting.\n",
    "\n",
    "data_path = pathlib.Path.cwd() / 'data'\n",
    "data_reports = [x for x in data_path.rglob('**/*') if x.suffix == '.csv']\n",
    "\n",
    "dataframe = pandas.DataFrame(columns=['location', 'timestamp'])\n",
    "for x in data_reports:\n",
    "    stamp = datetime.datetime.fromtimestamp(x.stat().st_mtime).strftime('%Y-%m-%d')\n",
    "    dataframe.loc[len(dataframe)] = [str(x), str(stamp)]\n",
    "\n",
    "for x in sorted(dataframe.timestamp.unique()):\n",
    "    daily = dataframe.copy()\n",
    "    daily = daily.loc[daily.timestamp.isin([x])]\n",
    "    comp = pandas.DataFrame()\n",
    "    for y in daily.location.unique():\n",
    "\n",
    "        comp = pandas.concat([comp, pandas.read_csv(str(y))])\n",
    "     \n",
    "\n",
    "    comp = comp.loc[comp.wikidata_actor.str.contains('Q', na=False)]\n",
    "    comp['character_name'] = comp.apply(character_length, axis=1)\n",
    "    comp = comp.replace({'character_name':{\n",
    "        'himself': 'NO CHARACTER', \n",
    "        'Himself': 'NO CHARACTER', \n",
    "        'herself': 'NO CHARACTER', \n",
    "        'Herself': 'NO CHARACTER', \n",
    "        'nan': 'NO CHARACTER', \n",
    "        None: 'NO CHARACTER', \n",
    "        numpy.nan: 'NO CHARACTER'}})\n",
    "\n",
    "    save_path = pathlib.Path.cwd() / 'statements' / f'{str(x)}.txt'\n",
    "    with open(save_path, 'w') as export:\n",
    "        for a in range(len(comp)):\n",
    "            row = comp.iloc[a]\n",
    "            statement_string = f'|{row[\"wikidata_film\"]}|P161|{row[\"wikidata_actor\"]}'\n",
    "            if row['character_name'] != 'NO CHARACTER':\n",
    "                statement_string += f'|P4633|\"{row[\"character_name\"]}\"'\n",
    "            statement_string += f'|S143|Q328\\n'\n",
    "            export.write(statement_string)\n",
    "\n",
    "    print(save_path.name, len(daily), len(comp))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-12-09.txt 4 53\n",
      "2021-12-10.txt 1244 14077\n",
      "2021-12-11.txt 2936 32864\n",
      "2021-12-12.txt 3382 37226\n",
      "2021-12-13.txt 407 4010\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}